\documentclass[hideweeklyreports,noposter]{polytech/polytech}
\usepackage{lmodern}
\usepackage{textcomp}

\usepackage{float}
\usepackage{ltablex}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

\usepackage{dirtree}

\floatplacement{figure}{H}
\floatplacement{table}{H}

\newacronym{ua}{UA}{user-agent}

\schooldepartment{di}
\typereport{pasrdi5}
\reportyear{2018-2019}

\title{Crawling web et requête HTTP par serveur proxy}
%\subtitle{}

\student{Thomas}{Couchoud}{thomas.couchoud@etu.univ-tours.fr}
\student{Victor}{Coleau}{victor.coleau@etu.univ-tours.fr}
\academicsupervisor{Mathieu}{Delalandre}{mathieu.delalandre@univ-tours.fr}

\resume{Crawler les sites web est de plus en plus difficile étant donné que les moyens de défense se multiplient. Nous allons ici étudier plusieurs techniques afin d'essayer de rendre son crawler inaperçu}
\motcle{crawler,scrapping,web,user-agent,proxy}

\abstract{Crawling websites became more and more difficult as means of defence are increasing. We will study different ways to try to make our crawler unnoticed from the websites.}
\keyword{crawler,scrapping,web,user-agent-proxy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\part{Introduction}
	Dans notre société moderne l'Internet occupe une place très importante.
	Il offre une quantité pharaonique d'information en libre accès.
	Parmi ces sources d'information, on trouve notamment des << wikis >> qui sont des encyclopédies collaboratives permettant la large diffusion de données.
	
	Malgré leur apparente générosité et leur connaissance des pratiques, ces sites n'apprécient que peu que les données qu'ils fournissent en soient extraites.
	
	Cette nouvelle mane d'information attire les convoitises et polarise les comportements.
	D'un côté nous retrouvons les collecteurs de données cherchant à en agréger et stocker de plus en plus de leur propre chef.
	De l'autre les sites mettant à disposition l'information dont le but paradoxal est de fournir gratuitement tout en conservant l'exclusivité.
	
	Cela entraine une guerre technologique entre crawlers et sites web.
	Les premiers développent des technologies de plus en plus efficaces, rapides et discrètes.
	Les seconds cherchent à contrecarrer les premiers grâce à des techniques de détection de plus en plus sophistiquées.

	Le but de ce projet est d'étudier à la fois les techniques mises en place par les crawler pour se rendre invisible et celles mise en place par les sites pour se défendre.
	Cette recherche se concrétisera par la réalisation d'un crawler effectuant ses connexions au travers d'un proxy.

	\img{crawler.png}{Crawler}{scale=0.5}
	
\part{Veille technique}
	\chapter{Stratégies de défense}
		Afin de se défendre face aux demandes massives que peuvent représenter les crawlers, les créateurs de sites web ont imaginé plusieurs méthodes de contre-attaque.
		Dans cette partie nous allons en développer quelques-unes.
		
		\section{Liste noire\label{sec:liste_noire}}
			Le principe de base d'une liste noire est de bannir du site les << utilisateurs >> trop agressifs.
			Un << utilisateur >> peut être un compte inscrit sur le site ou plus simplement une adresse IP requêtant le serveur.	
			
			La problématique principale de cette méthode est de différencier un utilisateur humain d'un automate.
			Afin de prendre la décision de bannir ou non, plusieurs stratégies sont à disposition des administrateurs:
			
			\begin{easylist}[itemize]
				@ User-agent: Le \gls{ua} est un champ renseigné dans l'entête d'une requête HTTP.
				Son but est d'identifier l'outil qui a engendré cette demande.
				Par exemple un \gls{ua} de Firefox ressemble à << Mozilla/5.0 (X11; Ubuntu; Linux\_x86\_64; rv:62.0) Gecko/20100101 Firefox/62.0 >> tandis que celui d'un bot Google est de la forme << Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) >>.
				
				Grace à cette identification, il est possible de filtrer les requêtes pour n'accepter que celles provenant des navigateurs web standards.
				
				@ Adresse IP: Une approche peut être de se baser sur l'adresse IP permettant d'identifier une machine ou un réseau précis.
				
				@ Comportement de l'utilisateur: A partir des méthodes d'identifications précédentes, il est possible de définir une stratégie de bannissement plus juste.
				En effet bannir tous les navigateurs qui ne sont pas Internet Explorer ou bien toutes les adresses IP commençant par 1 n'est que peu pertinent.
				
				Il est alors possible d'étudier les méthodes d'exploration du site afin de dénicher les comportements indésirables.
				Ces derniers peuvent être plus ou moins simples à observer.
				@@ Un nombre de requêtes humainement impossible (ex: 5 requêtes par seconde).
				@@ Des temps de visite des pages très courts (ex: 0.5 secondes par page).
				@@ Une fréquence de requêtage régulière (ex: toutes les secondes).
				@@ Des sauts de pages à d'autres non reliées par hyperlien.
			\end{easylist}
			~\\
			
			La pertinence de cette méthode de bannissement dépend grandement de la qualité de reconnaissance des automates.
			En effet, il se peut qu'un automate imite parfaitement le comportement humain, et donc ne soit pas détecté, tout comme un comportement humain inhabituel pourrait être banni par erreur.
			L'objectif est donc de développer des méthodes de reconnaissance adaptées afin de ne pas perdre des visiteurs désirés.
			
		\section{CAPTCHA}
			Le CAPTCHA est une famille de tests de Turing ayant pour but de différencier un utilisateur humain d'un automate.
			Plusieurs types de CPATCHAs sont imaginables:
			\begin{easylist}[itemize]
				@ Reconnaitre une site de lettres altérées (ex: déformées, barrées, avec des trous, ...).
				@ Cocher une case qui vérifie le comportement précédent de l'utilisateur.
				@ Reconnaitre des parties spécifiques d'une image.
			\end{easylist}
			
			\begin{figure}[H]
				\begin{minipage}{0.5\textwidth}
					\img{recaptcha-3.jpg}{Lettres altérées}{scale=0.5}
					\img{recaptcha-1.jpg}{Case à cocher}{scale=0.5}
				\end{minipage}
				\begin{minipage}{0.49\textwidth}
					\img{recaptcha-2.png}{Reconnaissance d'image}{scale=0.5}
				\end{minipage}
				\caption{Exemples de reCAPTCHAs}
			\end{figure}

		\section{Modification du DOM}
			Une autre approche serait non plus d'essayer de rejeter les automates mais de leur compliquer la tâche.
			
			Une méthode possible consiste à modifier constamment et régulièrement la structure du site (nom des classes, IDs, ...).
			De même, il est envisageable de remplacer certaines parties du site (notamment le texte) par des images.
			
			De ce fait les automates spécifiques à un site donné devront être réadaptés fréquemment ce qui peut décourager leur développeur.

        \section{Attrapes-bots}
            Afin de distinguer un humain d'un bot, il serait envisageable de baser notre différenciation sur l'exploration ou non d'une page web normalement non-visible d'un utilisateur lambda.
            Html / CSS offrent la possibilité de masquer certains éléments tout en les incluant au sein du code source de la page.
            
            Cependant, un crawler innatentif ne fera pas la différence entre des éléments visibles ou non.
            On pourrait donc renforcer la sécurité du site grâce à des liens pigés et bannir toute IP s'y rendant.
            
    \chapter{Outils d'analyse}
    	Lors de la réalisation de notre crawler et des stratégies de masquage qu'il implémente, nous avons dû analyser les traces qu'il laisse sur un site.
    	Pour cela nous avons mis en place un serveur apache et lancé notre crawler dessus.
    	
    	Nous allons ici présenter deux manières d'analyser l'activité des clients d'un serveur.
    	
		\section{Logs}
			La manière la plus simple de voir le traffic de notre site est de regarder les logs.
			
			\bashsourcefile{CrawlingLogs/uaRapport.log}
			
			L'exemple ci-dessus est un extrait de l'un de ces logs.
			On y retrouve les informations suivantes:
			\begin{easylist}[itemize]
				@ Adresse IP du client.
				@ Le nom de domaine.
				@ L'heure.
				@ Le type de requête (GET, POST, ...).
				@ La page demandée.
				@ Le protocol (HTTP/1.1).
				@ Le code HTTP retourné (200, 404, 403, 500, ...).
				@ La taille de la réponse.
				@ Origine de la requête.
				@ \gls{ua}.
			\end{easylist}

			Grâce à celles-ci nous pouvons identifier deux des caractéristiques citées plus haut (\gls{ua} et IP) du client.
			
			Nous pouvons, à partir d'un tel fichier, induire des informations plus poussées.
			Par exemple il est possible de recréer le chemin parcouru par l'utilisateur grâce à son adresse IP, \gls{ua} et temps de la requête.
			
			Du fait de sa grossièreté, le fichier de log est peu pratique à lire et interpréter.
			En revanche, un algorithme performant pourra repérer les utilisations incohérentes, impossibles et/ou humainement infaisables.
			
		\section{Urchin}
			Afin de simplifier l'analyse des logs pour tout le monde, des outils spécifiques ce sont créés.
			Nous allons ici présenter Urchin v6 qui analyse les logs et offre des vues graphiques de nombreuses métriques.
			Ces dernières peuvent être obtenues sur différentes périodes (journée, mois, période définie, ...).
			
			\img{u1.png}{Nombre de visites}{}
			
			Sur ces représentations graphiques nous pouvons facilement analyser les pics de fréquentation.
			Ceux-ci peuvent être associés à l'arrivée d'un bot.
			
			\img{u2.png}{Temps de visite}{}
			
			Ce schéma nous illustre les temps de visites par page.
			Si un pic entre 0-10 secondes se fait ressentir, il est possible que cela soit dû à un automate.
			
			\img{u3.png}{Provenance des visites}{}
			
			Dans le cas d'un site localisé (une langue précise par exemple), il peut être utile d'obtenir les origines des requêtes.
			Si un grand nombre sont effectuées depuis un pays extérieur, il est peu probable que cela soit normal.
			
			\img{u4.png}{Robots/User-Agents}{}
			\img{u5.png}{Robots/User-Agents précis}{}
			
			Sur cette page nous pouvons observer les \glspl{ua} qui nous ont visités.
			Si l'on en repère un étrange et récurrent, il faut alors se poser des questions.
			Il est aussi possible d'obtenir plus de détails sur chaque robot.

	\chapter{Stratégies de masquage}
		Comme évoqué précédemment, les gestionnaires de sites web tentent constamment de bloquer l'accès à leur contenu au travers de diverses méthodes.
		Ces dernières reposent principalement sur le bloquage d'un identifiant unique à la personne suite au repérage d'un comportement suspect.
		
		Les personnes attaquant les sites ont donc pour but de contourner ces mesures, soit en ayant plusieurs identifiants à leur disposition soit en essayant d'adopter un comportement proche d'un utilisateur normal.
		
		\section{Comportement du crawler}
            Une première mesure qui pourrait être envisagée afin d'éviter que le crawler soit identifié est de mofidifer les headers de requête HTTP par défaut mis en place par les librairies permettant d'effectuer des demandes HTTP.
            En effet, si l'on prend par exemple la librairie urllib de Python, on remarque que le \gls{ua} par défaut est << Python-urllib/3.4 >> et que le Accept-Encoding est << identity >>.
            Avec un tel \gls{ua}, le modérateur peut facilement comprendre qu'un programme Python est à l'origine de ces requêtes et non un utilisateur humain utilisant un navigateur web standard.
            Il serait alors plus judicieux de remplacer notre \gls{ua} par celui d'un navigateur web répendu (Chrome, Firefox, Edge, etc.).
            
            De même, le Accept-Encoding peut, dans certains cas, compromettre l'anonymat du crawler.
            Cependant, la modification de certaines informations du header peut entrainer des changements sur le site en question.
            Par exemple, le header Accept-Langage indique les préférences utilisateur concernant la langue. 
            Il se peut donc qu'un site mette à disposition deux versions en fonction de la langue souhaitée.
            
            
            Une deuxième méthode serait de prendre en charge les cookies au travers de JavaScript.
            En effet, ceux-ci peuvent être utilisés afin de reconnaitre un utilisateur ayant visité le site peu de temps auparavant.
            Si un crawler se rend sur le site de manière intensive sans présenter ce cookie, le serveur pourrait se rendre compte qu'il ne s'agit pas d'un utilisateur faisant des requêtes succesives.
            
            Cepndant, enregistrer les cookies, peut aussi jouer en notre défaveur : ceux-ci sont un façon de garder un trace d'un utilisateur précis et donc de l'identifier (Voir \autoref{sec:liste_noire}).
            
            
            Enfin, un trop grand nombre de requêtes en très peu de temps trahit un comportement robotique.
            Il est donc important de controler la vitesse de notre crawler afin de ne pas surcharger la bande passante du site et risquer de se faire bannir.
            
            Cette recommandation est en opposition avec les techniques de parrallèlisation souvent mises en place afin d'accélérer la vitesse et l'efficacité du crawler.
            
		
		\section{Utilisation de plusieurs identifiants}
			Comme vu précédemment, les principales techniques anti-crawler reposent sur le bannissement d'adresse IP irrespectueuses.
			Une méthode de contournement basique est donc de changer régulièrement d'adresse IP.
			Les proxys sont donc une alternative parfaite (Voir \autoref{sec:proxys}).



			De plus, certains sites utilisent des sessions ou comptes utilisateurs.
			Un crawler pourrait donc en tirer parti en s'indentifiant officiellement sur le site afin de se faire passer pour un utilisateur standard.

			Il est d'ailleurs possible de créer et d'utiliser plusieurs comptes en parallèle afin de répartir la charge de requêtes.

	\chapter{Les proxy\label{sec:proxys}}
		\section{Késako?}
			Un proxy est un composant logiciel servant d'intermédiaire entre deux hôtes afin de faciliter ou de surveiller leurs échanges.
			
			\img{proxy.png}{La place d'un proxy dans une communication}{}
			
			\subsection{Avantages}
				Les proxys peuvent par exemple servir à contourner certains filtrages.
				Supposons le cas d'un pays qui bloque l'accès à certains sites, en se connectant à un proxy non bloqué, l'utilisateur pourra accéder à son site au travers de ce dernier car le proxy ne dispose pas des mêmes règles de filtrage.
				
				A l'inverse, certains établissements scolaires ou entreprises limitent l'accès à certains sites grace à un serveur proxy. En effet, toutes les requêtes effectuées par les utilisateurs du réseau passe par ce serveur intermédiaire qui bloque les sites dont l'adresse a été spécifiquement interdite.
				
				Un autre avantage de l'utilisation d'un proxy est de pouvoir surfer anonymement. Les sites visités n'ont conscience que de l'adresse du proxy et non de(s) utilisateur(s) caché(s) derrière.
				
				De plus un proxy permet le masquage de son lieu de connexion. En effet le proxy peut ne pas être situé dans le même pays que l'utilisateur.
				Si un site se base sur un système de géolocalisation pour afficher son contenu (YouTube, Google, Google Maps, etc.), sera prise en compte la géolocalisation du proxy.
				
			\subsection{Inconvénients}
				Bien que les poxys offrent de nombreux avantages, ils ont aussi certains inconvénients.
				S'agissant d'une plateforme reliant un utilisateur au web et effectuant les requêtes du premier, celui-ci voit passer tous les échanges.
				Certains proxys pourraient alors les enregistrer à des fins malveillantes.
				
				L'intérêt d'un proxy étant important, il centralise toutes les requêtes d'une structure et peut donc être saturé ralentissant par la même occasion la connexion de tous les utilisateurs.
				De manière générale on peut dire qu'une connexion internet passant par un proxy sera toujours plus lente qu'une connexion directe.
				
		\section{Alternative - VPN}
			VPN est un acronyme signifiant << Virtual Private Network >>.
			Tout comme les proxys ils permettent de faire apparaître notre navigation internet comme provenant d'une adresse IP distante.
			
			A la différence d'un proxy qui se configure par application au cas par cas (par exemple dans Firefox on peut avoir une configuration différente de celle dans Chrome).
			En revanche un VPN capture l'ensemble des échanges réseau de la machine et est configuré directement dans le système d'exploitation.
			Les différentes applications de la machine n'ont donc pas conscience de cette subtilité.
			
			\img{vpn.png}{La place d'un VPN dans une communication}{}
			
			Dans le cas de notre crawler, il parait plus adapté d'utiliser un proxy.
			En effet seul ce dernier requiert d'être << masqué >> et passer toute notre connexion au travers d'un VPN semble être un peu overkill.
			
		\section{Présentation de proxys}
			\begin{center}
				\centering
				\begin{tabularx}{\textwidth}{|X|X|X|X|X|}
					\hline
					Nom & Prix & Pays & Avantages & Inconvénients\\\hline
					\href{https://proxy6.net/}{Proxy6} & 1.25\$ par IPv4 par mois & Un choix très large & API pour développeurs \& Le moins cher & \\\hline
					\href{https://buyproxies.org/}{BuyProxies} & 2\$ par serveur proxy par mois & Un choix très large & Bande passante illimité \& renouvellement mensuel des proxys & \\\hline
					\href{https://getfoxyproxy.org/}{FoxyProxy} & 10\$ par IP par mois & - & & Pays inconnu \& Le plus cher\\\hline
					\href{https://proxyrack.com/}{ProxyRack} & 40\$ pour 50 connexions simultanées par mois & Un choix moyen de pays & Rotation des adresses IP \& Bande passante illimitée & Premier pack cher\\\hline
				\end{tabularx}
				\captionof{table}{Caractéristiques utilisateurs}
			\end{center}
		
\part{Application: Réalisation d'une crawler avec proxy}
	\chapter{Crawler basique}
		Dans le cadre de ce projet nous devons tester les stratégies de masquage adoptables par un crawler.
		Afin de maitriser au mieux ces tests, nous avons souhaité développer nous même un tel logiciel.
		
		Afin d'obtenir un résultat de crawling pertinent tout en restant simple d'implémentation, nous avons choisi de se concentrer sur la recherche et téléchargement des images.
		Pour ce faire, notre crawler explore tous les liens d'une page ainsi que toutes les images.
		Il téléchargera les images trouvées tandis qu'il continuera d'explorer les liens d'un même domaine.
		
		Dans l'optique de garder le crawler simple, le javascript n'est pas supporté et seuls les liens dans les balises << a >> et << img >> sont traités.
		
		\section{Méthode implémentée}
			Dans un premier temps, les problématiques de bloquage ne seront pas prises en compte, seule l'efficacité prime.
			Pour cela, nous avons pensé à un système supportant le multi-threading augmentant la capacité d'acquisition des données.
			
			Le programme a été découpé en deux parties: les crawlers et les downloaders.
			
			Les crawlers sont en charge de l'exploration du site web et remplissent deux queues.
			La première contient les prochains liens à explorer et la seconde les liens des images à télécharger.
			
			Les downloaders, eux, ne font que lire les liens de la queue qui leur est associée et télécharge les images.
			
			L'utilisation de ces queues nous permet de séparer l'exploration et le téléchargement dans des threads différents mais aussi d'avoir plusieurs crawlers et plusieurs downloaders.
			
			\subsection{Crawlers}
				Le travail d'un crawler se décompose de la manière suivante:
				\begin{easylist}
					@ Acquisition d'un lien depuis la queue des liens.
					Si aucun lien n'est disponible, le thread s'endort temporairement et reprendra au début.
					@ Ajout de ce lien à la liste des liens déjà explorés.
					@ Téléchargement de la page HTML.
					@ Lecture de cette page et récupération des différents liens des balises << a >> et << img >>.
					@ Tri des liens obtenus:
					@@ Si le lien correspond à une image (basé sur l'extension): on vérifie que cette image n'a pas déjà été téléchargée.
					Si tel est le cas, on l'ajoute dans la queue des images (queue sans doublons).
					@@ Sinon: il vérifie si le lien a déjà été exploré.
					Si ce n'est pas le cas on vérifie qu'il fait partie du même domaine que le lien en cours d'exploration.
					Si tel est le cas on l'ajoute dans la queue des liens à explorer (queue sans doublons).
				\end{easylist}
			
			\subsection{Downloaders}
				Le travail d'un downloader se décompose de la manière suivante:
				\begin{easylist}
					@ Acquisition d'un lien depuis la queue des images.
					Si aucun lien n'est disponible, le thread s'endort temporairement et reprendra au début.
					@ Ajout de ce lien à la liste des images déjà téléchargées.
					@ Récupération du nom de l'image à partir du lien.
					@ Acquisition du contenu:
					@@ Si le ficher de sortie est déjà présent, on ne fait que renseigner le lien dans un ficher texte au nom de l'image (contiendra tous les liens menant potentiellement à cette dernière).
					@@ Sinon, on télécharge l'image.
				\end{easylist}
				
		\section{Résultats de tests}
			\begin{center}
				\centering
				\begin{tabularx}{\textwidth}{|X|X|}
					\hline
					Site & Observations\\\hline
					\href{https://en.wikipedia.org/wiki/Main_Page}{Wikipedia} & Aucune restriction\\\hline
					\href{https://www.qwertee.com/}{Qwertee} & Aucune restriction\\\hline
					\href{https://www.etam.com/}{Etam} & Aucune restriction\\\hline
					\href{https://www.google.com/}{Google} & Bloqué assez rapidement\\\hline
					\href{https://www.4chan.org/}{4chan} & Aucune restriction\\\hline
					\href{https://tinder.com/}{Tinder} & Nécessite une authentification + JavaScript\\\hline
					\href{https://www.prettylittlethings.fr/}{PrettyLittleThings} & JavaScript\\\hline
					\href{https://www.ladechetterieduweb.com/}{LaDechetterieDuWeb} & JavaScript\\\hline
				\end{tabularx}
				\captionof{table}{Résultats sur différents sites}
			\end{center}
			
			D'après nos tests effectués sur plusieurs sites, nous pouvons remarquer que les principaux freins au crawling sont les sites nécessitant une inscription ou ceux ayant un chargement dynamique grâce à du JavaScript.
	
	\chapter{Utilisation de stratégies de masquage}
		Afin de faire ressembler au mieux les requêtes de notre crawler à celles d'un navigateur standard nous avons décidé d'imiter le plus fidèlement possible leur header.
		Pour cela nous avons commencé par inspecter les données que reçois le serveur suite à la navigation depuis Firefox.
		
		\phpsourcefile{req.php}
		
		Avec ce code nous obtenons le résultat suivant:
		\img{headFirefox.png}{Paramètres de la requête GET}{}
			
		\section{User agent}
			La première chose a été de changer le \gls{ua}. Etant donné que nous utilisons une librairie (Unirest), il nous suffit de passer en paramètre nos headers.
			Ces derniers sont basés sur les tests précédents:
			\begin{easylist}[itemize]
				@ Le \gls{ua}
				@ Accept
				@ Aceept-Encoding
				@ Accept-Language
				@ Cache-Control
			\end{easylist}

			Après un test réalisé en local, nous avons pu confirmer que les headers parviennent bien au serveur et le User-Agent est bien celui défini dans le code.
			
			Les logs Apache étaient auparavant:
			\bashsourcefile{CrawlingLogs/basicRapport.log}
			
			Ils sont maintenant:
			\bashsourcefile{CrawlingLogs/uaRapport.log}
			 
		\section{Limite de requêtage}
			La deuxième étape consiste à limiter le nombre de requêtes envoyées au serveur.
			Nous avons imaginé deux méthodes: limiter chaque thread ou limiter l'ensemble de ces derniers.
			
			Dans les deux cas nous faisons une surcharge de ConcurrentLinkedQueue et n'avons plus qu'à gérer le délais d'accès.
			En effet la classe mère gère déjà les accès concurrents.
			
			\subsection{Sur l'ensemble}
				Cette classe s'intitule  LimitedGlobalConcurrentLinkedQueue.
				Dans celle-ci nous avons redéfini la méthode poll permettant d'obtenir le prochain élément.
				
				Le procédé est le suivant:
				\begin{easylist}[itemize]
					@ On calcule le $\Delta t$ séparant le temps de notre demande d'accès et le temps du dernier accès à une valeur.
					@ Si ce $\Delta t$ est inférieur à un $\Delta_r$ de référence on endort le thread pour une durée de $\Delta_r - \Delta t$.
					
					Ce temps écoulé, le thread réitérera sa demande.
					@ Si $\Delta t$ est supérieur à $\Delta_r$ on met à jour le temps du dernier accès et donnons le prochain élément.
				\end{easylist}

				Grâce à cette implémentation nous arrivons à passer plus inaperçu auprès du serveur crawlé.
				Cependant on induit un temps d'attente global très fort ralentissant \textit{de facto} l'exécution de notre programme.
			
			\subsection{Par thread}
				Cette classe s'intitule  LimitedThreadConcurrentLinkedQueue.
				Dans celle-ci nous avons redéfini la méthode poll permettant d'obtenir le prochain élément.
				
				Le procédé est le suivant:
				\begin{easylist}[itemize]
					@ On calcule le $\Delta t$ séparant le temps de notre demande d'accès et le temps du dernier accès du thread en cours à une valeur.
					@ Si ce $\Delta t$ est inférieur à un $\Delta_r$ de référence on endort le thread pour une durée de $\Delta_r - \Delta t$.
					@ A son réveil (si il y eut), on met à jour le temps du dernier accès et donnons le prochain élément.
				\end{easylist}

				En comparaison à la méthode globale, celle-ci est largement plus rapide mais de fait moins discrète.
				
			\subsection{Etude des logs}
				Les logs pour la version bloquant l'ensemble sont:
				\bashsourcefile{CrawlingLogs/globalBlockingRapport.log}
				
				Les logs pour la version bloquant chaque thread sont:
				\bashsourcefile{CrawlingLogs/threadBlockingRapport.log}
				
				Dans les deux cas, nous n'avons gardé que 2 secondes de log.
				On peut déjà remarquer que la version bloquant les threads individuellement envoie beaucoup plus de requêtes.
				
				Au nombre de log par seconde, on voit bien que la version bloquant l'ensemble des threads est beaucoup plus limitante et passe un peu plus inaperçue auprès des sites.
				Dans le cas où on souhaiterait crawler un site particulièrement vigilant il est fortement recommandé d'utiliser la version la moins aggressive du crawler.
	
	\chapter{Proxy}
	    Au vu des éléments précédemment évoqués, un moyen semblant particulièrement efficace de masquer un crawler serait de changer régulièrement d'adresse IP.
	    De ce fait, des comportements pouvant être suspects pour une seule adresse ne le seront plus une fois répartis sur plusieurs.
	    
	    Une méthode simple et efficace serait donc de passer par un ou plusieurs proxy.
	    Tel qu'est développé actuellement notre programme, il serait facile d'imaginer que chaque machine du cluster proxy exécute un seul thread.
	    Dans ce cas, chaque thread aurait donc une adresse IP d'origine différente des autres et les liens visités ne seraient alors plus vu comme venant d'un même utilisateur.
	    
	    
	    Il est à noter tout de même qu'avec une telle procédure, le crawler ne serait pas protéger contre une analyse poussée du schéma de visite du site.
	    En effet, les liens crawlés par les threads étant répartit de manière pseudo-aléatoire entre thread, aucun d'entre-eux n'aurait de suite cohérente de lien à visiter.
	    Il serait alors plus que probable que des sauts de pages normalement impossibles se réalise pouvant indiquer au site qu'il ne s'agit pas d'un visiteur normal.
		

\part{Conclusion}

    En conclusion, ce projet porta sur deux aspects fondamentalement opposés bien qu'étroitement liés de la diffusion d'information sur internet : la collecte de masse face à une diffusion controlée.
    
    Comme étudié dans ce rapport, les créateurs de contenu ont à leur disposition plusieurs techniques afin de contrer les utilisateurs peu scrupuleux collectant de grandes quantités de données.
    L'objectif final reste toujours le même repérer des comportements étranges afin de les identifier comme ne venant pas de navigateur (et utilisateurs) conventionnels afin de bannir les adresses IP concernées.
    
    
    De l'autre coté, les personnes souhaitant amasser les informations ont deux principales possibilités pour passer outre ces protections : développer des crawlers respectueux ou tenter d'éviter les mécaniqmes de défense.
    
    Dans le premier cas, plusieurs détails peuvent permettre de ne pas déranger le serveur : tenir compte du fichier robot.txt, ralentir la vitesse de requêtage, etc.
    Dans le second cas, afin d'éviter de se faire repérer, il est possible de modifier les headers des requêtes, changer régulièrement d'adresse IP, suivre un schéma de visite, etc.
    Dans tous les cas l'objectif est de se rapprocher le plus possible d'un comportement d'un utilisateur.
    Ces comportements sont tout de même déconseillés.

%\appendix		
\end{document}