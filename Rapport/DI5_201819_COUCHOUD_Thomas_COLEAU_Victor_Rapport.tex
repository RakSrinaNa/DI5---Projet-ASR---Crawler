\documentclass[hideweeklyreports,noposter]{polytech/polytech}
\usepackage{lmodern}
\usepackage{textcomp}

\usepackage{float}
\usepackage{ltablex}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

\usepackage{dirtree}

\floatplacement{figure}{H}
\floatplacement{table}{H}

\RequirePackage[at]{easylist}
\AtBeginEnvironment{easylist}
        {\ListProperties(Progressive*=3ex, Start1=1)}
        {}
        {}

\newacronym{ua}{UA}{user-agent}

\newcommand{\img}[3]{%
	\begin{center}
		\centering%
		\IfFileExists{./Images/#1}{%
			\includegraphics[scale=#3]{./Images/#1}%
		}{%
			\PackageError{TODO}{Image #1 missing!}{#2}%
			$\boxed{Image\;#1\;missing}$%
		}%
		\captionof{figure}{#2}%
	\end{center}
}
\newcommand{\codec}[1]{\texttt{#1}}

\schooldepartment{di}
\typereport{pasrdi5}
\reportyear{2018-2019}

\title{Crawling web et requête HTTP par serveur proxy}
%\subtitle{}

\student{Thomas}{Couchoud}{thomas.couchoud@etu.univ-tours.fr}
\student{Victor}{Coleau}{victor.coleau@etu.univ-tours.fr}
\academicsupervisor{Mathieu}{Delalandre}{mathieu.delalandre@univ-tours.fr}

\resume{} %TODO
\motcle{} %TODO

\abstract{} %TODO
\keyword{} %TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\part{Introduction}
	Dans notre société moderne l'Internet occupe une place très importante.
	Il offre une quantité pharaonique d'information en libre accès.
	Parmi ces sources d'information, on trouve notamment des << wikis >> qui sont des encyclopédies collaboratives permettant la large diffusion de données.
	
	Malgré leur apparente générosité et leur connaissance des pratiques, ces sites n'apprécient que peu que les données qu'ils fournissent en soient extraites.
	
	Cette nouvelle mane d'information attire les convoitises et polarise les comportements.
	D'un côté nous retrouvons les collecteurs de données cherchant à en agréger et stocker de plus en plus de leur propre chef.
	De l'autre les sites mettant à disposition l'information dont le but paradoxal est de fournir gratuitement tout en conservant l'exclusivité.
	
	Cela entraine une guerre technologique entre crawlers et sites web.
	Les premiers développent des technologies de plus en plus efficaces, rapides et discrètes.
	Les seconds cherchent à contrecarrer les premiers grâce à des techniques de détection de plus en plus sophistiquées.

	Le but de ce projet est d'étudier à la fois les techniques mises en place par les crawler pour se rendre invisible et celles mise en place par les sites pour se défendre.
	Cette recherche se concrétisera par la réalisation d'un crawler effectuant ses connexions au travers d'un proxy.

	\img{crawler.png}{Crawler}{0.5}
	
\part{Veille technique}
	\chapter{Stratégies de défense}
		Afin de se défendre face aux demandes massives que peuvent représenter les crawlers, les créateurs de sites web ont imaginés plusieurs méthodes de contre-attaque.
		Dans cette partie nous allons en développer quelques unes.
		
		\section{Liste noire\label{sec:liste_noire}}
			Le principe de base d'une liste noire est de bannir du site les << utilisateurs >> trop agressifs.
			Un << utilisateur >> peut être un compte inscrit sur le site ou plus simplement une adresse IP requêtant le serveur.	
			
			La problématique principale de cette méthode est différencier un utilisateur humain d'un automate.
			Afin de prendre la décision de bannir ou non, plusieurs moyens sont à disposition des administrateurs:
			
			\begin{easylist}[itemize]
				@ User-agent: Le \gls{ua} est un champ renseigné dans l'entête d'une requête HTTP.
				Son but est d'identifier l'outil qui a engendré cette demande.
				Par exemple un \gls{ua} de Firefox ressemble à << Mozilla/5.0 (X11; Ubuntu; Linux\_x86\_64; rv:62.0) Gecko/20100101 Firefox/62.0 >> tandis que celui d'un bot Google est de la forme << Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) >>.
				
				Grace à cette identification, il est possible de filtrer les requêtes pour n'accepter que celles provenant des navigateurs web standards.
				
				@ Adresse IP: Une approche peut être de se baser sur l'adresse IP permettant d'identifier une machine ou un réseau précis.
				
				@ Comportement de l'utilisateur: A partir des méthodes d'identifications précédentes, il est possible de définir une stratégie de bannissement plus juste.
				En effet bannir tous les navigateurs qui ne sont pas Internet Explorer ou bien toutes les adresses IP commençant par 1 n'est que peu pertinent.
				
				Il est alors possible d'étudier les méthodes d'explorations du site afin de dénicher les comportement indésirables.
				Ces derniers peuvent être plus ou moins simples à observer.
				@@ Un nombre de requêtes humainement impossible (ex: 5 requêtes par seconde).
				@@ Des temps de visite des pages très courts (ex: 0.5 secondes par page).
				@@ Une fréquence de requêtage régulière (ex: toutes les secondes).
				@@ Les sauts de pages à d'autres non reliées par hyperlien.
			\end{easylist}
			~\\
			
			La pertinence de cette méthode de bannissement dépends grandement de la qualité de reconnaissance des automates.
			En effet, il se peut qu'un automate imite parfaitement le comportement humain, et donc ne soit pas détecté, tout comme un comportement humain inhabituel pourrait être banni par erreur.
			L'objectif est donc de développer des méthodes de reconnaissance adaptées afin de ne pas perdre des visiteurs désirés.
			
		\section{CAPTCHA}
			Le CAPTCHA est une famille de tests de Turing ayant pour but de différencier un utilisateur humain d'un automate.
			Plusieurs types de CPATCHAs sont imaginables:
			\begin{easylist}[itemize]
				@ Reconnaitre une site de lettres altérées (ex: déformées, barrées, avec des trous, ...).
				@ Cocher une case qui vérifie le comportement précédent de l'utilisateur.
				@ Reconnaitre des parties spécifiques d'une image.
			\end{easylist}
			
			\begin{figure}[H]
				\begin{minipage}{0.5\textwidth}
					\img{recaptcha-3.jpg}{Lettres altérées}{0.5}
					\img{recaptcha-1.jpg}{Case à cocher}{0.5}
				\end{minipage}
				\begin{minipage}{0.49\textwidth}
					\img{recaptcha-2.png}{Reconnaissance d'image}{0.5}
				\end{minipage}
				\caption{Exemples de reCAPTCHAs}
			\end{figure}

		\section{Modification du DOM}
			Une autre approche serait non plus d'essayer de rejeter les automates mais de leur compliquer la tâche.
			
			Une méthode possible consiste à modifier constamment et régulièrement la structure du site (nom des classes, IDs, ...).
			De même, il est envisageable de remplacer certaines parties du site (notamment le texte) par des images.
			
			De ce fait les automates spécifiques à un site donné devront être réadaptés fréquemment ce qui peut décourager leur développeur.

        \section{Attrapes-bots}
            Afin de distinguer un humain d'un bot, il serait envisageable de baser notre différenciation sur l'exploration ou non d'une page web normalement non-visible d'un utilisateur lambda.
            Html / CSS offrent la possibilité de masquer certains éléments tout en les incluant au sein du code source de la page.
            
            Cependant, un crawler innatentif ne fera pas la différence entre des éléments visibles ou non.
            On pourrait donc renforcer la sécurité du site grâce à des liens pigés et bannir toute IP s'y rendant.

	\chapter{Stratégies de masquage}
		Comme décris précédemment, les gestionnaires de sites web tentent constamment de bloquer l'accès à leur contenu au travers de diverses méthodes.
		Ces dernières reposent principalement sur le bloquage d'un identifiant unique à la personne suite au repérage d'un comportement suspect.
		
		Les personnes attaquant les sites ont donc pour but de contourner ces mesures, soit en ayant plusieurs identifiants à leur disposition soit en essayant d'adopter un comportement proche d'un utilisateur normal.
		
		\section{Comportement du crawler}
		
			Rotation des UserAgent
			Limitation du nombre de requêtes
			Réessai

            Une première mesure qui pourrait être envisagée afin d'éviter que le crawler soit identifié est de mofidifer les headers de requête HTTP par défaut mis en place par les librairies permettant d'effectuer des demandes HTTP.
            En effet, si l'on prend par exemple la librairie urllib de Python, on remarque que le \gls{ua} par défaut est << Python-urllib/3.4 >> et que le Accept-Encoding est << identity >>.
            Avec un tel \gls{ua}, le modérateur peut facilement comprendre qu'un programme Python est à l'origine de ces requêtes et non un utilisateur humain utilisant un navigateur web standard.
            Il serait alors plus judicieux de remplacer notre \gls{ua} par celui d'un navigateur web répendu (Chrome, Firefox, Edge, etc.).
            
            De même, le Accept-Encoding peut, dans certains cas, compromettre l'anonymat du crawler.
            Cependant, la modification de certaines informations du header peut entrainer des changements sur le site en question.
            Par exemple, le header Accept-Langage indique les préférences utilisateur concernant la langue. 
            Il se peut donc qu'un site mette à disposition deux versions en fonction de la langue souhaitée.
            
            
            Une deuxième méthode serait de prendre en charge les cookies au travers de JavaScript.
            En effet, ceux-ci peuvent être utilisés afin de reconnaitre un utilisateur ayant visité le site peu de temps auparavant.
            Si un crawler se rend sur le site de manière intensive sans présenter ce cookie, le serveur pourrait se rendre compte qu'il ne s'agit pas d'un utilisateur faisant des requêtes succesives.
            
            Cepndant, enregistrer les cookies, peut aussi jouer en notre défaveur : ceux-ci sont un façon de garder un trace d'un utilisateur précis et donc de l'identifier (Voir \autoref{sec:liste_noire}).
            
            
            Enfin, un trop grand nombre de requêtes en très peu de temps trahit un comportement robotique.
            Il est donc important de controler la vitesse de notre crawler afin de ne pas surcharger la bande passante du site et risquer de se faire bannir.
            
            Cette recommandation est en opposition avec les techniques de parrallèlisation souvent mises en place afin d'accélérer la vitesse et l'efficacité du crawler.
            
		
		\section{Utilisation de plusieurs identifiants}
		Rotation d'IP
		Gestion de sessions
				
	\chapter{Les proxy}
		\section{Késako?}
			Un proxy est un composant logiciel servant d'intermédiaire entre deux hôtes afin de faciliter ou de surveiller leurs échanges.
			
			\subsection{Avantages}
				Les proxys peuvent par exemple servir à contourner certains filtrages.
				Supposons le cas d'un pays qui bloque l'accès à certains sites, en se connectant à un proxy non bloqué, l'utilisateur pourra accéder à son site au travers de ce dernier car le proxy ne dispose pas des mêmes règles de filtrage.
				
				A l'inverse, certains établissements scolaires ou entreprises limitent l'accès à certains sites grace à un serveur proxy. En effet, toutes les requêtes effectuées par les utilisateurs du réseau passe par ce serveur intermédiaire qui bloque les sites dont l'adresse a été spécifiquement interdite.
				
				Un autre avantage de l'utilisation d'un proxy est de pouvoir surfer anonymement. Les sites visités n'ont conscience que de l'adresse du proxy et non de(s) utilisateur(s) caché(s) derrière.
				
				De plus un proxy permet le masquage de son lieu de connexion. En effet le proxy peut ne pas être situé dans le même pays que l'utilisateur.
				Si un site se base sur un système de géolocalisation pour afficher son contenu (YouTube, Google, Google Maps, etc.), sera prise en compte la géolocalisation du proxy.
				
			\subsection{Inconvénients}
				Bien que les poxys offrent de nombreux avantages, ils ont aussi certains inconvénients.
				S'agissant d'une plateforme reliant un utilisateur au web et effectuant les requêtes du premier, celui-ci voit passer tous les échanges.
				Certains proxys pourraient alors les enregistrer à des fins malveillantes.
				
				L'intérêt d'un proxy étant important, il centralise toutes les requêtes d'une structure et peut donc être saturé ralentissant par la même occasion la connexion de tous les utilisateurs.
				De manière générale on peut dire qu'une connexion internet passant par un proxy sera toujours plus lente qu'une connexion directe.
		
\part{Application: Réalisation d'une crawler avec proxy}
\part{Conclusion}

\appendix		
\end{document}