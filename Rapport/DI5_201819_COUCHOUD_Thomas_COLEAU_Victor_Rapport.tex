\documentclass[hideweeklyreports,noposter]{polytech/polytech}
\usepackage{lmodern}
\usepackage{textcomp}

\usepackage{float}
\usepackage{ltablex}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

\usepackage{dirtree}

\floatplacement{figure}{H}
\floatplacement{table}{H}

\newcommand{\img}[3]{%
	\begin{center}
		\centering%
		\IfFileExists{./Images/#1}{%
			\includegraphics[scale=#3]{./Images/#1}%
		}{%
			\PackageError{TODO}{Image #1 missing!}{#2}%
			$\boxed{Image\;#1\;missing}$%
		}%
		\captionof{figure}{#2}%
	\end{center}
}
\newcommand{\codec}[1]{\texttt{#1}}

\schooldepartment{di}
\typereport{pasrdi5}
\reportyear{2018-2019}

\title{Crawling web et requête HTTP par serveur proxy}
%\subtitle{}

\student{Thomas}{Couchoud}{thomas.couchoud@etu.univ-tours.fr}
\student{Victor}{Coleau}{victor.coleau@etu.univ-tours.fr}
\academicsupervisor{Mathieu}{Delalandre}{mathieu.delalandre@univ-tours.fr}

\resume{} %TODO
\motcle{} %TODO

\abstract{} %TODO
\keyword{} %TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\RequirePackage[ampersand]{easylist}
\AtBeginEnvironment{easylist}
        {\ListProperties(Progressive*=3ex, Start1=1)}
        {}
        {}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\part{Introduction}
	Dans notre société moderne l'Internet occupe une place très importante.
	Il offre une quantité pharaonique d'information en libre accès.
	Parmi ces sources d'information, on trouve notamment des << wikis >> qui sont des encyclopédies collaboratives permettant la large diffusion de données.
	
	Malgré leur apparente générosité et leur connaissance des pratiques, ces sites n'apprécient que peu que les données qu'ils fournissent en soient extraites.
	
	Cette nouvelle mane d'information attire les convoitises et polarise les comportements.
	D'un côté nous retrouvons les collecteurs de données cherchant à en agréger et stocker de plus en plus de leur propre chef.
	De l'autre les sites mettant à disposition l'information dont le but paradoxal est de fournir gratuitement tout en conservant l'exclusivité.
	
	Cela entraine une guerre technologique entre crawlers et sites web.
	Les premiers développent des technologies de plus en plus efficaces, rapides et discrètes.
	Les seconds cherchent à contrecarrer les premiers grâce à des techniques de détection de plus en plus sofistiquées.

	Le but de ce projet est d'étudier à la fois les techniques mises en place par les crawler pour se rendre invisible et celles mise en place par les sites pour se défendre.
	Cette recherche se concrétisera par la réalisation d'un crawler effectuant ses connexions au travers d'un proxy.

	\img{crawler.png}{Crawler}{0.5}
	
\part{Veille technique}
	\chapter{Les proxy}
		\section{Késako?}
			Un proxy est un composant logiciel servant d'intermédiaire entre deux hôtes afin de faciliter ou de surveiller leurs échanges.
			
			\subsection{Avantages}
				Les proxys peuvent par exemple servir à contourner certains filtrages.
				Supposons le cas d'un pays qui bloque l'accès à certains sites, en se connectant à un proxy non bloqué, l'utilisateur pourra accéder à son site au travers de ce dernier car le proxy ne dispose pas des mêmes règles de filtrage.
				
				A l'inverse, certains établissements scolaires ou entreprises limitent l'accès à certains sites grace à un serveur proxy. En effet, toutes les requêtes effectuées par les utilisateurs du réseau passe par ce serveur intermédiaire qui bloque les sites dont l'adresse a été spécifiquement interdite.
				
				Un autre avantage de l'utilisation d'un proxy est de pouvoir surfer anonymement. Les sites visités n'ont conscience que de l'adresse du proxy et non de(s) utilisateur(s) caché(s) derrière.
				
				De plus un proxy permet le masquage de son lieu de connexion. En effet le proxy peut ne pas être situé dans le même pays que l'utilisateur.
				Si un site se base sur un système de géolocalisation pour afficher son contenu (YouTube, Google, Google Maps, etc.), sera prise en compte la géolocalisation du proxy.
				
			\subsection{Inconvénients}
				Bien que les poxys offrent de nombreux avantages, ils ont aussi certains inconvénients.
				S'agissant d'une plateforme reliant un utilisateur au web et effectuant les requêtes du premier, celui-ci voit passer tous les échanges.
				Certains proxys pourraient alors les enregistrer à des fins malveillantes.
				
				L'intérêt d'un proxy étant important, il centralise toutes les requêtes d'une structure et peut donc être saturé ralentissant par la même occasion la connexion de tous les utilisateurs.
				De manière générale on peut dire qu'une connexion internet passant par un proxy sera toujours plus lente qu'une connexion directe.
			
		\section{Description technique}
		
	
	\chapter{Stratégies de masquage}
		Rotation d'IP
		Rotation des UserAgent
		Gestion de sessions
		Limitation du nombre de requêtes
		Réessai
		
	\chapter{Stratégies de défense}
		Liste noire d'IP/comptes => Analyse du traffic => Attention à ne pas bloquer les utilisateurs normaux
		CAPTCHA
		Changer la structure du DOM régulièrement
		Remplacer des parties du site par des images
		
		
		
\part{Application: Réalisation d'une crawler avec proxy}
\part{Conclusion}

\appendix		
\end{document}